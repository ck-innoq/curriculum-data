=== {learning-goals}


// tag::DE[]
[[LZ-7-1]]
==== LZ 7-1 - Unterscheidung von Queries
Die Teilnehmer:innen verstehen, dass (Daten-)Transformationen erforderlich sind, um Daten aus operativen Systemen für die Analyse aufzubreiten. Sie verstehen, dass Transformationen in Form von Queries beschrieben werden können, dass bei steigender Komplexität aber einzelne Queries zu unverständlich werden und Abfolgen von Queries (s. Data Pipelines in LZ 10) besser für die Umsetzung von Transformationen geeignet sind.

[[LZ-7-2]]
==== LZ 7-2 - Anwendungsfälle
Die Teilnehmer:innen kennen übliche Aufgaben, die von Transformationen übernommen werden, wie speziell

- Umwandeln der Datenrepräsentation von Quell- in Zielformate
- Bereinigen der Daten (Data Cleansing)
- Identifizieren und Entfernen von Duplikaten
- Ergänzung der Daten (Enhancement)
- Vereinheitlichung/Standardisierung von Daten mehrerer Quellsysteme, z.B. Lokalisierung
- Reduktion des Datenumfangs, Komprimierung
- Anonymisierung, Pseudonomisierung, Verschlüsselung

Die Teilnehmer:innen verstehen, wie Transformationen Verfahren des Query und Processing (s. LZ 6) nutzen, um auf die Daten in den Speichersystemen (s. LZ 5) zuzugreifen.

Die Teilnehmer:innen verstehen, wie Transformationen bei verteilten Daten parallel ausgeführt werden.

Die Teilnehmer:innen verstehen die Bedeutung von Schemas und Datenmodellen (s. LZ 9) für die Umsetzung von Transformationen.

[[LZ-7-3]]
==== LZ 7-3 - Typische Transformationen
Die Teilnehmer:innen kennen typische Transformationen für die Aufbereitung analytischer Daten, wie etwa

- Normalisierung / Denormalisierung von Datenstrukturen - Star/Snowflake Schema, Data Vault
- Zuordnung von künstlichen IDs (surrogate keys) zu IDs der Quellsysteme
- (Multidimensionale) Aggregation (Cubes)
- Umwandlung zwischen flachen und geschachtelten Datenstrukturen
- Historisieren (Slowly Changing Dimensions)
- Ersetzen / Aussortieren unwahrscheinlicher Werte / Datensätze (Outlier)

[[LZ-7-4]]
==== LZ 7-4 - Staging Area
Den Teilnehmer:innen ist das Konzept der Staging Area vertraut. Sie wissen, dass

- analytische Daten aus Datenquellen in einer Staging Area gesammelt und aufbereitet werden, bevor sie für die Analyse zur Verfügung gestellt werden
- Staging Areas typischerweise für die Batch Verarbeitung von Daten zum Befüllen eines DWH Systems eingesetzt werden
- das tatsächliche Befüllen des DWH keine komplexen Operationen mehr erfordert und üblicherweise ganz oder gar nicht erfolgt (atomar)
- die Staging Area sowohl das vollständige Verarbeiten der Daten einer Datenquelle (initial load) als auch das Verarbeiten der Änderungsdaten (incremental load) unterstützen muss.
- die Staging Area immer nur genau die Änderungsdaten erhält, die seit der letzten Batchverarbeitung in den Quellsystemen angefallen sind.

Die Teilnehmer:innen wissen, welche Transformationen typischerweise in einer Staging Area angewendet werden.

[[LZ-7-5]]
==== LZ 7-5 - Robuste Transformationen
Die Teilnehmer:innen verstehen, dass die Umsetzung von Transformationen oft fragil ist, da sich die zu verarbeitenden Daten von Ausführung zu Ausführung im Hinblick auf Umfang, Format oder Bedeutung ändern können. Die Teilnehmer:innen kennen Lösungsansätze, um Transformationen robust zu gestalten:

- Vermeiden der Umsetzung von Geschäftslogik
- Gewährleisten der Wiederholbarkeit - Idempotente Transformationen
- Verwendung von Standards
- Einsatz von Verfahren zur Code-Generierung für eine automatisierte Anpassung von Transformationen bei Schema-Änderungen
- Auswahl geeigneter Programmiermodelle

Die Teilnehmer:innen kennen Verfahren zum Data Profiling, um die zu erwartenden Daten für die Umsetzung der Transformationen vorab zu analysieren.

Die Teilnehmer:innen kennen Verfahren für die Überwachung (Monitoring) der Ausführung von Transformationen.

Die Teilnehmer:innen kennen das Konzept der Data Lineage für das Nachvollziehen der Abhängigkeiten zwischen Ein- und Ausgangsdaten von Transformationen.

[[LZ-7-6]]
==== LZ 7-6 - Qualitätsstufen
Die Teilnehmer:innen verstehen, dass über eine schrittweise Transformation eingehender Daten Qualitätsstufen (Quality-Gates) definiert werden können. Sie verstehen, dass damit je nach Art der Analyse Daten einer höheren (Clean Data) oder einer niedrigeren (Raw Data) herangezogen werden können.

Die Teilnehmer:innen verstehen, dass Staging Areas (s. LZ 7-4) ein Quality Gate darstellen, mit dem sichergestellt wird, dass nur qualitätsgesicherte Daten zur Analyse in einem Data Warehouse bereitgestellt werden.

Die Teilnehmer:innen verstehen, dass in einem Data Lake (s. LZ 5-6) Daten unterschiedlicher Qualitätsstufen gemeinsam aufbewahrt werden und dass es daher sinnvoll sein kann, Zonen unterschiedlicher Datenqualität (etwa Bronze, Silber, Gold) innerhalb von Data Lakes festzulegen (Medallion Architecture).

[[LZ-7-7]]
==== LZ 7-7 - Batch Verarbeitung
Die Teilnehmer:innen verstehen das Verfahren der Batch Verarbeitung (Batch Processing) für die Transformation von Daten. Ihnen ist bewusst, dass

- die Batch Verarbeitung üblicherweise dazu dient, eine größere Anzahl Datensätze aus mehreren Datenquellen in einem Schritt zu verarbeiten und die Ergebnisse in eine Datensenke zu schreiben
- alle (initial/complete load) oder nur die letzten Änderungen (incremental load) im Batch verarbeitet werden können
- die Batch-Verarbeitung idR zeitlich geplant (scheduled) und wenige Male je Tag ausgeführt wird
- eine Batch-Verarbeitung idR alle Datensätze verarbeitet oder (im Fehlerfalle) keine
- eine Batch-Verarbeitung üblicherweise die Konsistenz (s. LZ 5-3) der Daten in der Datensenke erhält
- eine Batch-Verarbeitung komplexe Operationen zur Datenverarbeitung enthalten kann

Die Teilnehmer:innen verstehen das Verfahren der Micro-Batch Verarbeitung von Daten []. Sie kennen den Unterschied zwischen Batch und Micro-Batch Verarbeitung (Batches werden nur manuell oder zeitlich angestoßen, Micro-Batches auch dann, wenn ausreichend Datensätze in der Datenquelle angefallen sind).

[[LZ-7-8]]
==== LZ 7-8 - Stream Verarbeitung
Die Teilnehmer:innen verstehen das Verfahren der Stream Verarbeitung (Stream Processing) von Daten. Sie wissen, dass Stream Verarbeitung auf der Basis des Event Streamings aufsetzt.

Die Teilnehmer:innen wissen, dass bei der Stream Verarbeitung

- die Daten mehrerer Streams miteinander zu einem weiteren Stream kombiniert werden können.
- Datensätze (etwa fehlerhafte oder unvollständige) im Stream voneinander getrennt und separat (in unterschiedlichen Streams) weiterverarbeitet werden können.
- bei der Verarbeitung von Daten im Stream üblicherweise auf komplexe Operationen verzichtet wird.

Die Teilnehmer:innen verstehen, dass das Schreiben von Daten aus einem Datenstrom meist idempotent gestaltet wird.

Die Teilnehmer:innen können zustandslose (stateless) und zustandsbehaftete (stateful) Stream Verarbeitung unterscheiden.

Die Teilnehmer:innen verstehen, dass Operationen nicht auf allen Datensätzen eines Streams erfolgen können, sondern immer nur auf einzelnen oder einer Gruppe von aufeinanderfolgenden Datensätzen. Sie kennen dazu das Konzept der Fenster (Window) Funktionen.

Die Teilnehmer:innen kennen Frameworks oder Tools für die Stream Verarbeitung wie

- Kafka Streams
- Apache Flink
- Pulsar Functions
- Spark Streaming
// end::DE[]

// tag::EN[]
[[LG-6-1]]
==== LG 6-1: Aspects and building blocks
tbd.

[[LG-6-2]]
==== LG 6-2: Central approaches
tbd.

[[LG-6-3]]
==== LG 6-3: Data Mesh
tbd.

[[LG-6-4]]
==== LG 6-4: Machine Learning
tbd.

[[LG-6-5]]
==== LG 6-5: Use Cases
tbd.

// end::EN[]

// tag::REMARK[]
[NOTE]
====
Die einzelnen Lernziele müssen nicht als einfache Aufzählungen mit Unterpunkten aufgeführt werden, sondern können auch gerne in ganzen Sätzen formuliert werden, welche die einzelnen Punkte (sofern möglich) integrieren.
====
// end::REMARK[]

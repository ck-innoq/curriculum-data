=== {learning-goals}

// tag::DE[]
[[LZ-4-1]]
==== LZ 4-1 - Erkennen von Änderungen
Den Teilnehmern sind Verfahren für das Identifizieren von Entitäten sowie für das Erkennen von Änderungen dieser Entitäten in Quellsystemen bekannt, wie etwa

- Change Data Capture (CDC)
- DB-Trigger
- Outbox Pattern
- Event-Sourcing
- Create und Update Timestamps
- Audit Trail Reader
- Datensatz-Hashes
- Primary Keys
- (RSS-)Feeds

Die Teilnehmer wissen, für welche Art von Quellsystemen welche dieser Verfahren jeweils geeignet sind.

Den Teilnehmern ist bewusst, dass das Löschen von Daten üblicherweise ebenfalls als Änderung geliefert wird, da auch
gelöschte Daten für die Analyse ggfs. weiter benötigt werden.

Den Teilnehmern ist bewusst, dass sich Änderungen von Entitäten auf den internen Zustand von Quellsystemen beziehen
können und dass mit der Weitergabe dieser Änderungen das Prinzip des Information Hidings verletzt werden kann. Sie
verstehen, wie etwa das Outbox Pattern dazu genutzt werden kann, die von Quellsystemen zur Verfügung gestellten
Änderungen auf das erforderliche Maß zu beschränken.

[[LZ-4-2]]
==== LZ 4-2 - Konnektoren
Die Teilnehmer kennen Tools und Frameworks, die über vordefinierte Konnektoren einen vereinheitlichten Zugriff auf
Quellsysteme zur Verfügung stellen. Sie wissen, dass diese Tools und Frameworks vorrangig für die folgenden Aufgaben
eingesetzt werden:

- Datenreplikation
- Datenintegration
- Datenvirtualisierung
- Datenorchestrierung
- Metadatenmanagement

Die Teilnehmer kennen Beispiele für diese Tools, wie etwa

- Fivetran
- Stitch
- funnel
- integrate.io
- talend
- airbyte
- Kafka Connect
- Pulsar IO
- Trino / Presto
- alluxio
- Collibra
- Alation

[[LZ-4-3]]
==== LZ 4-3 Eigenschaften von Data Ingestion

Die Teilnehmer wissen, was die Frequenz der Data Ingestion ist. Sie wissen auch, dass die Frequenz sich von der
Datenverarbeitung unterscheiden kann, sodass auf eine Stream Ingestion ein Batch Processing folgen kann.

Die Teilnehmer kennen die Unterschiede zwischen synchroner und asynchroner Ingestion und wissen, dass ältere ETL
Systeme typische Beispiele für eine synchronen Workflow sind.

Den Teilnehmern ist bewusst, dass der Durchsatz und die Skalierbarkeit ein wichtiges Merkmal für die Toleranz der
Data  Ingestion darstellt. Zum einen erzeugen Data Generation Sources meist nicht gleichmäßig ihre Daten
und zum anderen werden sie benötigt, falls sich die Requirements ändern oder wenn die Source Database nicht
erreichbar ist und die Daten nachgeholt werden müssen.

Die Teilnehmer können einschätzen, wie wichtig die Zuverlässigkeit und Beständigkeit für die Ingestion in einem
System sind und wissen, dass diese Eigenschaften mit einem hohen Tradeoff der Kosten durch Redundanz der Daten kommt.

Die Teilnehmer wissen, dass Datenquellen den aktuellen Zustand der zuletzt geänderten Daten (Snapshot) oder die
Änderungen selbst (Increments/Events) wiedergeben können, mit denen dieser aktuelle Zustand erzeugt wurde. Sie
wissen, dass ggfs. eine initiale Bereitstellung (initial Load) der Daten erforderlich ist.

Die Teilnehmer wissen, dass Datenquellen ihre Daten selbst aktiv liefern (Push) oder aber auf Anfrage bereitstellen
(Pull) können. Sie wissen hierzu was Push, Pull und Poll Patterns sind und wie sie sich unterscheiden.

[[LZ-4-4]]
==== LZ 4-4 Batch vs Stream Ingestion

Den Teilnehmern ist der grundlegende Unterschied zwischen Batch- und Stream-Verarbeitung bekannt.

Den Teilnehmern ist bewusst, wie Batch Ingestion durch zeitliche oder größen-basierte Abschnitte aus einem Streaming
Umfeld erstellt werden können.

Die Teilnehmer kennen typische Batch-basierte Ingestion Patterns:

- Snapshots oder inkrementelle Extraktion
- File-based export and ingestion
- ETL versus ELT
- Inserts, updates, and batch size
- Data migration

Den Teilnehmern ist bewusst, dass es bei spalten-basierten Datenbanken ein schlechtes Vorgehen ist reihen-basierte
Inserts zu machen.

[[LZ-4-5]]
==== LZ 4-5 TBD noch zuordnen

Den Teilnehmern ist das Risiko von zu spät ankommenden Daten bei der Stream Ingestion bewusst und dass sich dies
kritisch auf downstream Systeme auswirken kann.

Die Teilnehmer wissen, dass Daten, sowohl über eine direkte Datenbankverbindung, wie JDBC, als auch als Datei, aus
einer Datenbank extrahiert werden können.

[[LZ-3-3]]
==== LZ 3-3 - Metadaten
Die Teilnehmer verstehen die Bedeutung von Metadaten für die Nutzung von Datenquellen. Sie können technische und
fachliche Metadaten unterscheiden. Sie wissen, dass Quellsysteme oft umfassend Metadaten zu den von ihnen
bereitgestellten Datenquellen zur Verfügung stellen.

Die Teilnehmer wissen wie wichtig die Charakteristiken von Datasets sind. Sie wissen, dass folgende Informationen in
Form von Metadaten z.B. für Folgesysteme von großem Nutzen sind und daher einem Datensatz mitgegeben werden sollten:

- Typ (Tabelle, Bild, Text, ...)
- Dateiformat
- Dimension (Tabellen: Zeilen x Spalten, JSON: key-value Paare, Bilder: Größe in Pixeln x RGB)
- Größe (Bytes, Gigabytes, Terabytes, ...)
- Schema und Datentypen
- weitere Metadaten

Die Teilnehmer wissen, dass es Verfahren gibt (Schema Inference), um Metadaten aus Daten abzuleiten. Ihnen ist
bewusst, dass sich Metadaten über die Nutzungsdauer von Daten ändern können (Schema Evolution) und dass Verfahren
existieren, diese Änderungen beim Zugriff auf die Daten zu erkennen.

Die Teilnehmer kennen das Konzept eines Data Catalogs. Sie wissen, dass ein Data Catalog die Übersichtlichkeit
der darunter liegenden Daten deutlich erhöht und einen Einstiegspunkt für die Suche und Beziehungserkennung liefert.
Sie wissen außerdem, dass im Data Catalog die Data Lineage integriert werden kann, in welchem die Beziehungen der
Daten visualisiert werden.
// end::DE[]

// tag::EN[]
[[LG-4-1]]
==== LG 4-1: Categories of data consumers
tbd.

[[LG-4-2]]
==== LG 4-2: Data providers
tbd.

[[LG-4-3]]
==== LG 4-3: Data presentation patterns
tbd.

[[LG-4-4]]
==== LG 4-4: Predictive techniques
tbd.

[[LG-4-5]]
==== LG 4-5: Integration in operative systems
tbd.
// end::EN[]

// tag::REMARK[]
[NOTE]
====
Die einzelnen Lernziele müssen nicht als einfache Aufzählungen mit Unterpunkten aufgeführt werden, sondern können auch gerne in ganzen Sätzen formuliert werden, welche die einzelnen Punkte (sofern möglich) integrieren.
====
// end::REMARK[]

=== {learning-goals}


// tag::DE[]
[[LZ-10-1]]
==== LZ 10-1 - Was sind Data Pipelines
Die Teilnehmer:innen wissen, dass Data Pipelines dazu dienen analytische Daten durch die einzelnen Phasen des Data Engineerings zu bewegen. Sie wissen was typischerweise in den einzelnen Phasen geschieht (siehe LZ-1-7). Sie kennen die wesentlichen Eigenschaften von Data Pipelines wie:

- Isolation
- Unabhängigkeit
- Einfache Einrichtung und Betreibbarkeit
- hohe Verfügbarkeit
- Erweiterbarkeit
- Skalierbarkeit

Außerdem kennen die Teilnehmer:innen typische Anwendungsgebiete von Data Pipelines:

- Data Engineering
- Analytics/ML Processing
- Delivery

[[LZ-10-2]]
==== LZ 10-2 - Arten von Data Pipelines
Die Teilnehmer:innen wissen um die verschiedenen Arten von Data Pipelines. Sie wissen in welchen Situationen diese zum Einsatz kommen und wann sie vorteilhaft sind.

Die Teilnehmer:innen kennen die Unterschiede zwischen Batch (ETL und ELT), Microbatch, Stream und Eventgetriebener Verarbeitung.

[[LZ-9-3]]
==== LZ 9-3 - Data Pipeline Qualitätskriterien
Die Teilnehmer:innen kennen maßgebliche Qualitätskriterien, die die Güte einer Data Pipeline beschreiben, wie etwa:

- Durchsatz
- Zuverlässigkeit
- Latenz
- Stabilität

[[LZ-9-4]]
==== LZ 9-4 - Building Blocks von Data Pipelines
Die Teilnehmer:innen wissen aus welchen Building Blocks eine Data Pipelines besteht. Sie kennen Beispiele, wissen wann welche Tools für einen Anwendungsfall geeignet sind und wie diese kombiniert werden können.

- Scheduler
- Workflow Engine
- Message Broker
- Low Cost/High Volume Data Storage
- Stream Processing Engine
- Data Catalog

[[LZ-9-5]]
==== LZ 9-5 - Technologien/Tools für Data Pipelines
Die Teilnehmer:innen kennen Technologien, wie SQL, Python Dataframes oder Spark, mit denen Data Pipelines selbst erstellt werden können. Sie kennen aber auch Tools um Data Pipelines zu erstellen, wie z.B.:

- Apache Airflow
- Databricks
- Fivetran
- Astera Centerprise
- Skyvia

[[LZ-9-6]]
==== LZ 9-6 - Monitoring
Die Teilnehmer:innen wissen, dass für die Data Pipeline ein Monitoring notwendig ist, um die Performanz und Qualität der Verarbeitung sicherzustellen.

Die Teilnehmer:innen verstehen, dass das Monitoring dabei relevante Buisness Metriken umfassen muss, welche von der Anwendung abhängig sind und an diese angepasst werden müssen.

Den Teilnehmer:innen ist bewusst, dass in einer Cloud Umgebung zusätzliche Eigenschaften beobachtet werden müssen, wie z.B.:

- Latenz
- Häufigkeit von Systemfehlern
- Datenmenge
- Auslastung

[[LZ-9-7]]
==== LZ 9-7 - Optimization
Den Teilnehmer:innen ist bewusst, dass Data Pipelines sehr häufig automatisiert laufen und wissen, dass Optimierung daher eine große Rolle für Data Pipelines spielt. Die Teilnehmer:innen kennen daher Möglichkeiten eine Laufzeitanalyse für Data Pipelines aufzusetzen und wissen falls nötig dann wie sie die Laufzeiten der Operationen optimieren können.

[[LZ-9-8]]
==== LZ 9-8 - Cloud vs on-Premise
Den Teilnehmer:innen ist bewusst, dass Data Pipelines sowohl in on-Premise als auch in Cloud Umgebungen etabliert werden können.

Die Teilnehmer:innen kennen die Vor- und Nachteile von Cloud vs on-Premise Datenverarbeitung. Zusätzlich kennen sie die Wirkung von Edge-Computing und wissen wie sich diese auf die Datenverarbeitung auswirkt.

Die Teilnehmer:innen wissen insbesondere, dass Data Pipelines so gestaltet werden können, dass sie sowohl in der Cloud als auch on-premise ausführbar sind, sodass ein Setup ggfs. in das andere übertragen werden kann.
// end::DE[]

// tag::EN[]
[[LG-6-1]]
==== LG 6-1: Aspects and building blocks
tbd.

[[LG-6-2]]
==== LG 6-2: Central approaches
tbd.

[[LG-6-3]]
==== LG 6-3: Data Mesh
tbd.

[[LG-6-4]]
==== LG 6-4: Machine Learning
tbd.

[[LG-6-5]]
==== LG 6-5: Use Cases
tbd.

// end::EN[]

// tag::REMARK[]
[NOTE]
====
Die einzelnen Lernziele müssen nicht als einfache Aufzählungen mit Unterpunkten aufgeführt werden, sondern können auch gerne in ganzen Sätzen formuliert werden, welche die einzelnen Punkte (sofern möglich) integrieren.
====
// end::REMARK[]

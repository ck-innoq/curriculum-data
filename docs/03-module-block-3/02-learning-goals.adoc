=== {learning-goals}

// tag::DE[]
[[LZ-3-1]]
==== LZ 3-1 - Arten von Datenquellen und Quellsystemen
Die Teilnehmer:innen kennen Beispiele für inhaltliche und technische Arten von Datenquellen und Quellsystemen, von denen analytische Daten typischerweise übernommen werden, wie etwa:

inhaltliche Quellen
- Sensordaten
- Standard Anwendungen
- Logging Systeme
- (Web-) Analytics
- Metrics Systeme

technische Quellen
- (REST-) APIs
- Datenbanken
- Event-Feed
- Messaging Systeme
- (Cloud) Storage Systeme


[[LZ-3-2]]
==== LZ 3-2 - Eigenschaften von Datenquellen und Quellsystemen
Die Teilnehmer:innen können unterscheiden, ob Datenquellen strukturierte, semi-strukturierte oder unstukturierte Daten liefern. Ihnen sind übliche Formate vertraut, in denen Datenquellen ihre Daten zur Verfügung stellen, wie etwa

- Dokumente
- Liste von Datensätzen
- Key-value Paare

Den Teilnehmer:innen ist bewusst, dass Quellsysteme in unterschiedlicher Weise und Qualität den Schutz der Daten sowie eine Kontrolle des Zugriffs auf die Daten unterstützen.

Den Teilnehmer:innen ist bewusst, dass Quellsysteme z.T. sehr große Datenmengen bereitstellen, und dass geeignete Maßnahmen getroffen werden müssen, um diese Datenmengen transportieren zu können.

Die Teilnehmer:innen wissen, dass das zur Verfügung stellen analytischer Daten i.d.R. nicht der eigentliche Zweck von Quellsystemen ist. Sie wissen, dass dadurch zusätzliche Last für das jeweilige Quellsystem erzeugt werden kann und dass dies i.d.R. den eigentlichen Zweck eines Quellsystems nicht beeinträchtigen darf. Zusätzlich kann die Verfügbarkeit des Quellsystems eingeschränkt sein.

Die Teilnehmer:innen wissen, dass Daten normalisiert oder denormalisiert sein können und können entsprechend mit ihnen umgehen.

Den Teilnehmer:innen ist bewusst, dass die Daten in Quellsystemen partitioniert sein können und wissen, dass sie diese Partitionen für schnellere Abfragen nutzen können.

Den Teilnehmer:innen ist bewusst, dass es bei technologisch veralteten Quellsystemen schwieriger ist auf die Daten zuzugreifen.

Den Teilnehmer:innen ist bewusst, welche Garantien, wie Consistency oder eventual Consistency, eine Datenquelle zur Verfügung stellen kann.

Die Teilnehmer:innen wissen, dass eine Datenquelle z.B. eine SQL Schnittstelle oder vergleichbare Alternativen, zur Verfügung stellt, um auf die Daten zuzugreifen. Die Teilnehmer:innen wissen auch, ob eine Datenquelle einen Batch oder eine Streamverarbeitung der Daten unterstützt.

Die Teilnehmer:innen wissen einzuschätzen, ob eine real time oder near real time Verarbeitung der Daten notwendig ist oder nicht.

[[LZ-3-3]]
==== LZ 3-3 - Eigenschaften von Datenbanksystemen

Die Teilnehmer:innen wissen, dass die Datenmodelle von Quellsystemen veränderbar sind. Ein direkter Zugriff auf diese Datenmodelle koppelt die Analysesysteme an die Quellsysteme.

[[LZ-3-4]]
==== LZ 3-4 - Event Streaming Basics
Die Teilnehmer:innen wissen, dass

- beim Event Streaming je Datenquelle idR ein Producer für das Lesen der Daten aus dieser Quelle und das Weiterleiten an eine entsprechende Middleware Komponente (Message Broker) verantwortlich ist.
- beim Event Streaming je Datensenke idR ein Consumer für das Lesen der Daten vom Message Broker und das Schreiben in diese Senke verantwortlich ist.
- entsprechend die Daten einer Datenquelle von mehreren Consumern in mehrere Datensenken geschrieben werden können.
- Event Streaming nicht generell garantiert, dass jeder Datensatz genau einmal (exactly once) verarbeitet wird, sondern meist nur zusichert, dass jeder Datensatz höchstens (almost once) oder mindestens einmal (at least once) verarbeitet wird.
- mit Event Streaming idR nur eventuelle Konsistenz der Daten in der Datensenke zusichert wird.

[[LZ-3-5]]
==== LZ 3-5 - Eigenschaften vom Event-Streaming

Die Teilnehmer:innen wissen, dass Message Queues in einem System genutzt werden, um Messages mit bestimmten Qualitätskriterien zu übertragen. Streams hingegen stellen ein geordnetes Log der übertragenen Daten dar, welches verarbeitet werden kann.

Die Teilnehmer:innen wissen, dass die Types of Time bei der Streamverarbeitung eine besondere Rolle spielen, da sie die Zeitpunkte der Erstellung, Ingestion, den Start und die Dauer der Verarbeitung protokollieren.

Die Teilnehmer:innen wissen, dass die Eigenschaft der Häufigkeit der Übertragung für einen Stream eine große Rolle spielt. Im Speziellen wissen sie, dass die Idempotenz bei der Verarbeitung der Optimalfall ist.

Die Teilnehmer:innen wissen, dass eine Event Streaming Plattform sehr gut für Applikationen geeignet ist, welche schnell auf Input reagieren müssen oder um near-real time Analysen zu machen.

Die Teilnehmer:innen wissen, dass eine Message Queue keine großen Datenmengen (in Kilobytes) beinhaltet, sondern nur kleine individuelle Nachrichten zwischen Systemen bewegt.

Die Teilnehmer:innen wissen, dass ein Producer die einzelnen Events an sogenannte Topics sendet, welche eine Sammlung der zusammenhängenden Events darstellen. Beispiele wären Betrugswarnungen, Kundenbestellungen oder Temperaturangaben.

Den Teilnehmer:innen ist bewusst, dass Streams partitioniert werden können, sodass die Partitionen einzeln verarbeitet werden. Den Teilnehmer:innen ist dabei das Problem des hotspottings bekannt, bei dem zu viele Nachrichten einer Partition zugeordnet werden.

Die Teilnehmer:innen kennen den Vorteil der Fehlertoleranz von Streaming Systemen, bei dem Systeme die Streams über mehrere Nodes verteilen, sodass falls eine Node ausfällt der Stream immernoch erreichbar bleibt, da eine andere Node einspringt.
// end::DE[]

// tag::EN[]
[[LG-3-1]]
==== LG 3-1: General aspects
tbd.

[[LG-3-2]]
==== LG 3-2: Policies
tbd.

[[LG-3-3]]
==== LG 3-3: Metadata Management
tbd.

[[LG-3-4]]
==== LG 3-4: Data Quality
tbd.

[[LG-3-5]]
==== LG 3-5: Traceability
tbd.

[[LG-3-6]]
==== LG 3-6: Maintainability
tbd.
// end::EN[]



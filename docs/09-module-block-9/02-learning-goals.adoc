=== {learning-goals}


// tag::DE[]
[[LZ-9-1]]
==== LZ 9-1 - Was sind Data Pipelines
Die Teilnehmer:innen wissen, dass Data Pipelines dazu dienen analytische Daten durch die einzelnen Phasen des Data Engineerings zu bewegen. Sie wissen was typischerweise in den einzelnen Phasen geschieht (siehe LZ-1-7). Sie kennen die wesentlichen Eigenschaften von Data Pipelines wie:

- Isolation
- Unabhängigkeit
- Einfache Einrichtung und Betreibbarkeit
- hohe Verfügbarkeit
- Erweiterbarkeit
- Skalierbarkeit

Außerdem kennen die Teilnehmer:innen typische Anwendungsgebiete von Data Pipelines:

- Data Engineering
- Analytics/ML Processing
- Delivery

[[LZ-9-2]]
==== LZ 9-2 - Arten von Data Pipelines
Die Teilnehmer:innen wissen um die verschiedenen Arten von Data Pipelines. Sie wissen in welchen Situationen diese zum Einsatz kommen und wann sie vorteilhaft sind.

Die Teilnehmer:innen kennen die Unterschiede zwischen Batch (ETL und ELT), Microbatch, Stream und Eventgetriebener Verarbeitung.

[[LZ-9-3]]
==== LZ 9-3 - Data Pipeline Qualitätskriterien
Die Teilnehmer:innen kennen maßgebliche Qualitätskriterien, die die Güte einer Data Pipeline beschreiben, wie etwa:

- Durchsatz
- Zuverlässigkeit
- Latenz
- Stabilität

Die Teilnehmer:innen kennen Ansätze wie Idempotenz, Caching, Parallelisierung, Edge Computing um die Qualität von Data Pipelines zu verbessern.

[[LZ-9-4]]
==== LZ 9-4 - Building Blocks von Data Pipelines
Die Teilnehmer:innen wissen aus welchen Building Blocks eine Data Pipelines besteht und in welchen Phasen des Lebenszyklus diese verwendet werden. Sie kennen Beispiele dafür, wissen wann diese Building Blocks  für einen Anwendungsfall geeignet sind und wie diese kombiniert werden können.

- Konnektoren 
- Werkzeuge zur Ablaufsteuerung und Orchestrierung
- Kostengünstige Speichersysteme für hohe Datenvolumen
- Datenkataloge
- Werkzeuge zur Datentransformation
- Report-Generatoren

[[LZ-9-5]]
==== LZ 9-5 - Technologien und Plattformen für Data Pipelines
Die Teilnehmer:innen wissen wie Data Pipelines auf der Basis von Technologien, wie SQL, Python Dataframes oder Spark, manuell erstellt werden können. Sie kennen zudem auch integrierte Datenplattformen zur Erstellung und dem Management von Data Pipelines, wie z.B.:

- Databricks
- Fivetran
- Skyvia

[[LZ-9-6]]
==== LZ 9-6 - Betrieb von Data Pipelines
Den Teilnehmer:innen ist bewusst, dass für den Betrieb von Data Pipelines die folgenden Themen bedacht werden müssen:

- Monitoring
- Abhängigkeiten (Data Lineage)
- Metadaten
- Late arriving data
- Orchestrierung
- Schema Changes

// end::DE[]

// tag::EN[]
[[LG-6-1]]
==== LG 6-1: Aspects and building blocks
tbd.

[[LG-6-2]]
==== LG 6-2: Central approaches
tbd.

[[LG-6-3]]
==== LG 6-3: Data Mesh
tbd.

[[LG-6-4]]
==== LG 6-4: Machine Learning
tbd.

[[LG-6-5]]
==== LG 6-5: Use Cases
tbd.

// end::EN[]

// tag::REMARK[]
[NOTE]
====
Die einzelnen Lernziele müssen nicht als einfache Aufzählungen mit Unterpunkten aufgeführt werden, sondern können auch gerne in ganzen Sätzen formuliert werden, welche die einzelnen Punkte (sofern möglich) integrieren.
====
// end::REMARK[]
